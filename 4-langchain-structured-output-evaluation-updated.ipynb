{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating structured outputs in LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = \"<API KEY>\"\n",
    "ANTHROPIC_API_KEY = st.secrets[\"api_keys\"][\"ANTHROPIC_API_KEY\"]\n",
    "FIREWORKS_API_KEY = st.secrets[\"api_keys\"][\"FIREWORKS_API_KEY\"]\n",
    "experiment_date = \"06-02-25\"\n",
    "n_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt and problem setup\n",
    "\n",
    "For this test I’m going to start with a substitute task to write an article for a magazine and provide the response for different questions in a specific format.\n",
    "\n",
    "Here we specify the prompt and any inputs to use to vary the problem (the list of questions).0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_science_prompt_txt = \"\"\"\n",
    "You are a professional science writer tasked with responding to members of\n",
    "the general public who write in asking questions about science.\n",
    "Write an article responding to a writer's question for publication in a\n",
    "science magazine intended for a general readership with a high-school education.\n",
    "You should write clearly and compellingly, include all relavent context,\n",
    "and provide motivating stories where applicable.\n",
    "\n",
    "Your response must be less than 200 words.\n",
    "\n",
    "The question given to you is the following:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What is the oldest recorded fossil?\",\n",
    "    \"What is a black hole?\",\n",
    "    \"How far away is the sun?\",\n",
    "    \"Which other planet in the Solar System has a surface gravity closest to that of the Earth?\",\n",
    "    \"Eris, Haumea, Makemake and Ceres are all examples of what?\",\n",
    "    \"Why does earth have seasons? Do other planets exhibit seasons too?\",\n",
    "    \"What causes the aurora borealis?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"How do bees communicate?\",\n",
    "    \"What is the smallest unit of life?\",\n",
    "    \"How do plants make their own food?\",\n",
    "    \"Why do we dream?\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"How do volcanoes erupt?\",\n",
    "    \"What is the speed of light?\",\n",
    "    \"How do magnets work?\",\n",
    "    \"What is the purpose of DNA?\",\n",
    "    \"What are the different types of galaxies?\",\n",
    "    \"Why do some animals hibernate?\",\n",
    "    \"How do vaccines work?\",\n",
    "]\n",
    "\n",
    "prompt_direct = ChatPromptTemplate.from_template(test_science_prompt_txt)\n",
    "\n",
    "prompt_system_format = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query.\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", test_science_prompt_txt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_user_format = ChatPromptTemplate.from_template(\n",
    "    test_science_prompt_txt + \"\\n{format_instructions}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON output format specs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic structures\n",
    "\n",
    "To answer the question of how these models and output methods differ with different complexities of schema I’m defining four example schema in increasing order of complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple types\n",
    "class ArticleResponse1(BaseModel):\n",
    "    \"\"\"Structured article for publication answering a reader's question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    answer: str = Field(\n",
    "        description=\"Provide a detailed description of historical events to answer the question.\"\n",
    "    )\n",
    "    number: int = Field(\n",
    "        description=\"An arbitraty number that is most relevant to the question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Lists of simple types\n",
    "class ArticleResponse2(BaseModel):\n",
    "    \"\"\"Structured article for publication answering a reader's question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    further_questions: list[str] = Field(\n",
    "        description=\"A list of related questions that may be of interest to the readers.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Nested types\n",
    "class HistoricalEvent(BaseModel):\n",
    "    \"\"\"The year and explanation of a historical event.\"\"\"\n",
    "\n",
    "    year: int = Field(description=\"The year of the historical event\")\n",
    "    description: str = Field(\n",
    "        description=\"A clear description of what happened in this event\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ArticleResponse3(BaseModel):\n",
    "    \"\"\"Structured article for publication answering a reader's question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    historical_event_1: HistoricalEvent = Field(\n",
    "        description=\"Provide a detailed description of one historical events to answer the question.\"\n",
    "    )\n",
    "    historical_event_2: HistoricalEvent = Field(\n",
    "        description=\"Provide a detailed description of one historical events to answer the question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Lists of custom types\n",
    "class ArticleResponse4(BaseModel):\n",
    "    \"\"\"Structured article for publication answering a reader's question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    historical_timeline: list[HistoricalEvent] = Field(\n",
    "        description=\"Provide a compelling account of the historical context of the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Nested types\n",
    "class CriicalAnalysis(BaseModel):\n",
    "    \"\"\"A critique of interpretations of historical events\"\"\"\n",
    "\n",
    "    historical_event: HistoricalEvent = Field(\n",
    "        description=\"Provide an overview of the facts of a historical event\"\n",
    "    )\n",
    "    common_understanding: str = Field(description=\"Agreed interpretation of event\")\n",
    "    analysis: str = Field(\n",
    "        description=\"Critical analysis of the event and opposing interpretations\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Multiple nested custom types\n",
    "class ArticleResponse5(BaseModel):\n",
    "    \"\"\"Structured article for publication answering a reader's question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    historical_timeline: list[HistoricalEvent] = Field(\n",
    "        description=\"Provide a compelling account of the historical context of the question\"\n",
    "    )\n",
    "    critique: list[CriicalAnalysis] = Field(\n",
    "        description=\"A list of key historical events and historical analysis of them\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_formats = [\n",
    "    dict(pydantic=ArticleResponse1),\n",
    "    dict(pydantic=ArticleResponse2),\n",
    "    dict(pydantic=ArticleResponse3),\n",
    "    dict(pydantic=ArticleResponse4),\n",
    "    dict(pydantic=ArticleResponse5),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default temperature\n",
    "temperature = 0.0\n",
    "timeout = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = {\n",
    "    \"Anthropic_Sonnet\": ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        api_key=claude_api_key,\n",
    "        request_timeout=timeout,\n",
    "    ),\n",
    "    \"Anthropic_Haiku\": ChatAnthropic(\n",
    "        model=\"claude-3-5-haiku-20241022\", api_key=claude_api_key\n",
    "    ),\n",
    "    \"Anthropic_Haiku\": ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\", api_key=claude_api_key, request_timeout=timeout\n",
    "    ),\n",
    "    \"Ollama_llama32\": ChatOllama(\n",
    "        model=\"llama3.2\", temperature=temperature, request_timeout=timeout\n",
    "    ),\n",
    "    \"Ollama_nemotron\": ChatOllama(\n",
    "        model=\"nemotron-mini\", temperature=temperature, request_timeout=timeout\n",
    "    ),\n",
    "    \"Ollama_phi3\": ChatOllama(\n",
    "        model=\"phi3\", temperature=temperature, request_timeout=timeout\n",
    "    ),\n",
    "    \"Ollama_phi4\": ChatOllama(\n",
    "        model=\"phi4\", temperature=temperature, request_timeout=timeout\n",
    "    ),\n",
    "    \"Ollama_deepseekr1\": ChatOllama(\n",
    "        model=\"deepseek-r1\", temperature=temperature, request_timeout=timeout\n",
    "    ),\n",
    "    \"fireworks_llama31\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "        api_key=FIREWORKS_API_KEY,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "    ),\n",
    "    \"fireworks_llama32\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/llama-v3p2-3b-instruct\",\n",
    "        api_key=FIREWORKS_API_KEY,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "    ),\n",
    "    \"fireworks_llama33\": ChatFireworks(\n",
    "        model_name=\"accounts/fireworks/models/llama-v3p3-70b-instruct\",\n",
    "        api_key=FIREWORKS_API_KEY,\n",
    "        temperature=temperature,\n",
    "        request_timeout=timeout,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's loop over different structured outputs and check the adherence using the tool-calling API (structured output mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Structured Ouputs accross providers & models\n",
    "\n",
    "Question - of the models that have tool calling, what complexity of structure can they support?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(prompt_format, questions, llm_models, method, n_iter=1):\n",
    "\n",
    "    structure_support_by_model = {}\n",
    "    n_questions = len(questions)\n",
    "\n",
    "    # Iterate over models\n",
    "    for model_name, llm_model in llm_models.items():\n",
    "        structure_support_by_model[model_name] = {}\n",
    "\n",
    "        # Iterate over schemas\n",
    "        for structure in structured_formats:\n",
    "            pydantic_obj = structure[\"pydantic\"]\n",
    "            print(f\"Model: {model_name}  Output: {pydantic_obj.__name__}\")\n",
    "\n",
    "            # Format instructions if required\n",
    "            parser = PydanticOutputParser(pydantic_object=pydantic_obj)\n",
    "            prompt = prompt_format.partial(\n",
    "                format_instructions=parser.get_format_instructions()\n",
    "            )\n",
    "\n",
    "            # Iterate over questions\n",
    "            error_types = []\n",
    "            error_messages = []\n",
    "            outputs = []\n",
    "            output_valid = 0\n",
    "            for _ in range(n_iter):\n",
    "                for ii in range(n_questions):\n",
    "                    try:\n",
    "                        test_chain = prompt | llm_model.with_structured_output(\n",
    "                            pydantic_obj, method=method, include_raw=True\n",
    "                        )\n",
    "                        output = test_chain.invoke(dict(question=questions[ii]))\n",
    "                        outputs.append(output)\n",
    "\n",
    "                        # Typically Pydantic validation failure\n",
    "                        if output[\"parsing_error\"] is not None:\n",
    "                            error_types.append(\"parse_error\")\n",
    "                            error_messages.append(output[\"parsing_error\"])\n",
    "                            print(\"Error: Parse error\")\n",
    "\n",
    "                        # Typically function-calling failure\n",
    "                        elif output[\"parsed\"] is None:\n",
    "                            error_types.append(\"no_output\")\n",
    "                            print(\"Error: No output\")\n",
    "\n",
    "                        # This is not expected to happen\n",
    "                        elif not isinstance(output[\"parsed\"], pydantic_obj):\n",
    "                            raise RuntimeError(\"Unexpected error\")\n",
    "\n",
    "                        else:\n",
    "                            output_valid += 1\n",
    "\n",
    "                    # Other failures (typically function-calling not supported)\n",
    "                    except Exception as e:\n",
    "                        error_types.append(\"other error\")\n",
    "                        print(f\"Error: Other error {type(e).__name__}\")\n",
    "                        error_messages.append(f\"{type(e).__name__}, {e}\")\n",
    "\n",
    "                    # Pause to avoid timeouts\n",
    "                    print(\".\", end=\"\")\n",
    "                    sleep(1)\n",
    "                print()\n",
    "\n",
    "            structure_support_by_model[model_name][pydantic_obj.__name__] = dict(\n",
    "                valid=output_valid / (n_iter * n_questions),\n",
    "                error_types=error_types,\n",
    "                errors=error_messages,\n",
    "                outputs=outputs,\n",
    "            )\n",
    "\n",
    "    return structure_support_by_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ollama_llama32  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse3\n",
      "Error: Parse error\n",
      ".Error: Parse error\n",
      ".\n",
      "Model: Ollama_llama32  Output: ArticleResponse4\n",
      "Error: Parse error\n",
      ".Error: Parse error\n",
      ".\n",
      "Model: Ollama_llama32  Output: ArticleResponse5\n",
      "Error: Parse error\n",
      ".Error: Parse error\n",
      ".\n",
      "Model: Ollama_nemotron  Output: ArticleResponse1\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: Ollama_nemotron  Output: ArticleResponse2\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: Ollama_nemotron  Output: ArticleResponse3\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: Ollama_nemotron  Output: ArticleResponse4\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: Ollama_nemotron  Output: ArticleResponse5\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: Ollama_phi3  Output: ArticleResponse1\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi3  Output: ArticleResponse2\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi3  Output: ArticleResponse3\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi3  Output: ArticleResponse4\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi3  Output: ArticleResponse5\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi4  Output: ArticleResponse1\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi4  Output: ArticleResponse2\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi4  Output: ArticleResponse3\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi4  Output: ArticleResponse4\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_phi4  Output: ArticleResponse5\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse1\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse2\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse3\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse4\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse5\n",
      "Error: Other error ResponseError\n",
      ".Error: Other error ResponseError\n",
      ".\n",
      "Model: fireworks_llama31  Output: ArticleResponse1\n",
      "..\n",
      "Model: fireworks_llama31  Output: ArticleResponse2\n",
      "..\n",
      "Model: fireworks_llama31  Output: ArticleResponse3\n",
      "..\n",
      "Model: fireworks_llama31  Output: ArticleResponse4\n",
      "..\n",
      "Model: fireworks_llama31  Output: ArticleResponse5\n",
      "..\n",
      "Model: fireworks_llama32  Output: ArticleResponse1\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: fireworks_llama32  Output: ArticleResponse2\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: fireworks_llama32  Output: ArticleResponse3\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: fireworks_llama32  Output: ArticleResponse4\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: fireworks_llama32  Output: ArticleResponse5\n",
      "Error: No output\n",
      ".Error: No output\n",
      ".\n",
      "Model: fireworks_llama33  Output: ArticleResponse1\n",
      "..\n",
      "Model: fireworks_llama33  Output: ArticleResponse2\n",
      ".Error: Other error BadGatewayError\n",
      ".\n",
      "Model: fireworks_llama33  Output: ArticleResponse3\n",
      "..\n",
      "Model: fireworks_llama33  Output: ArticleResponse4\n",
      "..\n",
      "Model: fireworks_llama33  Output: ArticleResponse5\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "structure_support_by_model_fc = run_experiment(\n",
    "    prompt_direct, questions, llm_models, method=\"function_calling\", n_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ollama_llama32  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse3\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse4\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse5\n",
      "..\n",
      "Model: Ollama_nemotron  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_nemotron  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_nemotron  Output: ArticleResponse3\n",
      "..\n",
      "Model: Ollama_nemotron  Output: ArticleResponse4\n",
      "..\n",
      "Model: Ollama_nemotron  Output: ArticleResponse5\n",
      "..\n",
      "Model: Ollama_phi3  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_phi3  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_phi3  Output: ArticleResponse3\n",
      "..\n",
      "Model: Ollama_phi3  Output: ArticleResponse4\n",
      "..\n",
      "Model: Ollama_phi3  Output: ArticleResponse5\n",
      "..\n",
      "Model: Ollama_phi4  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_phi4  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_phi4  Output: ArticleResponse3\n",
      "..\n",
      "Model: Ollama_phi4  Output: ArticleResponse4\n",
      "..\n",
      "Model: Ollama_phi4  Output: ArticleResponse5\n",
      "..\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse2\n",
      "..\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse3\n",
      "..\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse4\n",
      "..\n",
      "Model: Ollama_deepseekr1  Output: ArticleResponse5\n",
      "..\n",
      "Model: fireworks_llama31  Output: ArticleResponse1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized method argument. Expected one of 'function_calling' or 'json_mode'. Received: 'json_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m structure_support_by_model_js \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_direct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson_schema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(prompt_format, questions, llm_models, method, n_iter)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_questions):\n\u001b[0;32m---> 28\u001b[0m         test_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpydantic_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m             output \u001b[38;5;241m=\u001b[39m test_chain\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28mdict\u001b[39m(question\u001b[38;5;241m=\u001b[39mquestions[ii]))\n",
      "File \u001b[0;32m~/Code/mastering-structured-output/.venv/lib/python3.12/site-packages/langchain_fireworks/chat_models.py:1001\u001b[0m, in \u001b[0;36mChatFireworks.with_structured_output\u001b[0;34m(self, schema, method, include_raw, **kwargs)\u001b[0m\n\u001b[1;32m    995\u001b[0m     output_parser \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    996\u001b[0m         PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mschema)  \u001b[38;5;66;03m# type: ignore[type-var, arg-type]\u001b[39;00m\n\u001b[1;32m    997\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_pydantic_schema\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m JsonOutputParser()\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized method argument. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction_calling\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Received: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_raw:\n\u001b[1;32m   1007\u001b[0m     parser_assign \u001b[38;5;241m=\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m   1008\u001b[0m         parsed\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m output_parser, parsing_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m _: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized method argument. Expected one of 'function_calling' or 'json_mode'. Received: 'json_schema'"
     ]
    }
   ],
   "source": [
    "structure_support_by_model_js = run_experiment(\n",
    "    prompt_direct, questions, llm_models, method=\"json_schema\", n_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ollama_llama32  Output: ArticleResponse1\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse2\n",
      "Error: Parse error\n",
      "..\n",
      "Model: Ollama_llama32  Output: ArticleResponse3\n",
      "Error: Parse error\n",
      "."
     ]
    }
   ],
   "source": [
    "structure_support_by_model_jm = run_experiment(\n",
    "    prompt_user_format, questions, llm_models, method=\"json_mode\", n_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleResponse1</th>\n",
       "      <th>ArticleResponse2</th>\n",
       "      <th>ArticleResponse3</th>\n",
       "      <th>ArticleResponse4</th>\n",
       "      <th>ArticleResponse5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ArticleResponse1  ArticleResponse2  ArticleResponse3  \\\n",
       "Ollama_nemotron             100.0             100.0             100.0   \n",
       "\n",
       "                 ArticleResponse4  ArticleResponse5  \n",
       "Ollama_nemotron             100.0             100.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_support_by_model = structure_support_by_model_js\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        mname: {\n",
    "            tname: structure_support_by_model[mname][tname][\"valid\"] * 100\n",
    "            for tname in structure_support_by_model[mname].keys()\n",
    "        }\n",
    "        for mname in structure_support_by_model.keys()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(ss_results, key=\"valid\"):\n",
    "    df = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            mname: {\n",
    "                tname: ss_results[mname][tname][key] * 100 / n_questions\n",
    "                for tname in ss_results[mname].keys()\n",
    "            }\n",
    "            for mname in ss_results.keys()\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyse_errors_from_results(ss_results, method=\"code\"):\n",
    "    error_counts = {}\n",
    "    for mname in ss_results.keys():\n",
    "        error_counts[mname] = {}\n",
    "        for tname in ss_results[mname].keys():\n",
    "            validation_error = 0\n",
    "            json_error = 0\n",
    "            unknown_error = 0\n",
    "\n",
    "            # Count errors by failure code above\n",
    "            if method == \"code\":\n",
    "                error_types = pd.Series(ss_results[mname][tname][\"error_types\"])\n",
    "                error_codes = error_types.value_counts()\n",
    "\n",
    "                for e_name, e_count in error_codes.items():\n",
    "                    error_counts[mname][(tname, e_name)] = e_count\n",
    "\n",
    "            elif method == \"parse\":\n",
    "                # Count errors by parsing error message\n",
    "                errors = ss_results[mname][tname][\"errors\"]\n",
    "                for error in errors:\n",
    "                    error_str = str(error)\n",
    "                    if error_str.lower().find(\"invalid json output\") >= 0:\n",
    "                        json_error += 1\n",
    "                    elif error_str.lower().find(\"validation error\") >= 0:\n",
    "                        validation_error += 1\n",
    "                    else:\n",
    "                        unknown_error += 1\n",
    "                error_counts[mname][(tname, \"invalid_json\")] = json_error\n",
    "                error_counts[mname][(tname, \"validation\")] = validation_error\n",
    "                error_counts[mname][(tname, \"unknown\")] = unknown_error\n",
    "\n",
    "            else:\n",
    "                raise NameError(f\"Method {method} not supported\")\n",
    "\n",
    "    return pd.DataFrame.from_dict(error_counts, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ArticleResponse4</th>\n",
       "      <th>ArticleResponse5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>parse_error</th>\n",
       "      <th>parse_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ArticleResponse4 ArticleResponse5\n",
       "                     parse_error      parse_error\n",
       "Ollama_nemotron                1                4"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df = analyse_errors_from_results(structure_support_by_model_jm)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">ArticleResponse1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ArticleResponse2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ArticleResponse3</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ArticleResponse4</th>\n",
       "      <th colspan=\"3\" halign=\"left\">ArticleResponse5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>invalid_json</th>\n",
       "      <th>validation</th>\n",
       "      <th>unknown</th>\n",
       "      <th>invalid_json</th>\n",
       "      <th>validation</th>\n",
       "      <th>unknown</th>\n",
       "      <th>invalid_json</th>\n",
       "      <th>validation</th>\n",
       "      <th>unknown</th>\n",
       "      <th>invalid_json</th>\n",
       "      <th>validation</th>\n",
       "      <th>unknown</th>\n",
       "      <th>invalid_json</th>\n",
       "      <th>validation</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ArticleResponse1                    ArticleResponse2  \\\n",
       "                    invalid_json validation unknown     invalid_json   \n",
       "Ollama_nemotron                0          0       0                0   \n",
       "\n",
       "                                   ArticleResponse3                     \\\n",
       "                validation unknown     invalid_json validation unknown   \n",
       "Ollama_nemotron          0       0                0          0       0   \n",
       "\n",
       "                ArticleResponse4                    ArticleResponse5  \\\n",
       "                    invalid_json validation unknown     invalid_json   \n",
       "Ollama_nemotron                0          0       0                0   \n",
       "\n",
       "                                    \n",
       "                validation unknown  \n",
       "Ollama_nemotron          0       0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df = analyse_errors_from_results(structure_support_by_model_fc, method=\"parse\")\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors_df = analyse_errors_from_results(structure_support_by_model_js)\n",
    "errors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Function-calling'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'JSON Schema'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'JSON Mode'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_list = {\n",
    "    \"Function-calling\": structure_support_by_model_fc,\n",
    "    \"JSON Schema\": structure_support_by_model_js,\n",
    "    \"JSON Mode\": structure_support_by_model_jm,\n",
    "}\n",
    "\n",
    "df_results = {}\n",
    "for name, ss_results in results_list.items():\n",
    "    df_results[name] = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            mname: {\n",
    "                tname: ss_results[mname][tname][\"valid\"] * 100\n",
    "                for tname in ss_results[mname].keys()\n",
    "            }\n",
    "            for mname in ss_results.keys()\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    display(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ArticleResponse1</th>\n",
       "      <th>ArticleResponse2</th>\n",
       "      <th>ArticleResponse3</th>\n",
       "      <th>ArticleResponse4</th>\n",
       "      <th>ArticleResponse5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Function-calling</th>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JSON Schema</th>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JSON Mode</th>\n",
       "      <th>Ollama_nemotron</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ArticleResponse1  ArticleResponse2  \\\n",
       "Function-calling Ollama_nemotron               0.0               0.0   \n",
       "JSON Schema      Ollama_nemotron             100.0             100.0   \n",
       "JSON Mode        Ollama_nemotron             100.0             100.0   \n",
       "\n",
       "                                  ArticleResponse3  ArticleResponse4  \\\n",
       "Function-calling Ollama_nemotron               0.0               0.0   \n",
       "JSON Schema      Ollama_nemotron             100.0             100.0   \n",
       "JSON Mode        Ollama_nemotron             100.0              80.0   \n",
       "\n",
       "                                  ArticleResponse5  \n",
       "Function-calling Ollama_nemotron               0.0  \n",
       "JSON Schema      Ollama_nemotron             100.0  \n",
       "JSON Mode        Ollama_nemotron              20.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(df_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| level_0          | level_1         |   ArticleResponse1 |   ArticleResponse2 |   ArticleResponse3 |   ArticleResponse4 |   ArticleResponse5 |\n",
      "|:-----------------|:----------------|-------------------:|-------------------:|-------------------:|-------------------:|-------------------:|\n",
      "| Function-calling | Ollama_nemotron |                  0 |                  0 |                  0 |                  0 |                  0 |\n",
      "| JSON Schema      | Ollama_nemotron |                100 |                100 |                100 |                100 |                100 |\n",
      "| JSON Mode        | Ollama_nemotron |                100 |                100 |                100 |                 80 |                 20 |\n"
     ]
    }
   ],
   "source": [
    "import tabulate\n",
    "\n",
    "print(\n",
    "    tabulate.tabulate(\n",
    "        df.reset_index(), headers=\"keys\", tablefmt=\"pipe\", showindex=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(file=f\"exp5_summary_df_{experiment_date}.json\", mode=\"wb\") as f:\n",
    "    df.to_json(f)\n",
    "\n",
    "with open(file=f\"exp5_all_models_{experiment_date}.pkl\", mode=\"wb\") as f:\n",
    "    pickle.dump(\n",
    "        dict(\n",
    "            structure_support_by_model_fc=structure_support_by_model_fc,\n",
    "            structure_support_by_model_jm=structure_support_by_model_jm,\n",
    "            structure_support_by_model_js=structure_support_by_model_js,\n",
    "        ),\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded structure_support_by_model\n",
      "Loaded structure_support_by_model_op\n",
      "Loaded structure_support_by_model_op_system\n",
      "Loaded structure_support_by_model_op_jsonmode\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(file=f\"exp5_summary_df_{experiment_date}.json\", mode=\"rb\") as f:\n",
    "    df = pd.read_json(f)\n",
    "\n",
    "with open(file=f\"exp5_all_models_{experiment_date}.pkl\", mode=\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Inject into toplevel namespace\n",
    "namespace = locals()\n",
    "for key, value in data.items():\n",
    "    if key not in namespace:\n",
    "        print(f\"Loaded {key}\")\n",
    "        namespace[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output parsers [Deprecated]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the output parser formatting. Note that as a lot of models ignore these instructions, it can take a lot of time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "def run_experiment_with_op(prompt_format, llm_models, n_iter):\n",
    "    ss_results = {}\n",
    "    n_questions = len(questions)\n",
    "\n",
    "    for model_name, llm_model in llm_models.items():\n",
    "        ss_results[model_name] = {}\n",
    "        for structure in structured_formats:\n",
    "            pydantic_obj = structure[\"pydantic\"]\n",
    "            print(f\"Model: {model_name}  Output: {pydantic_obj.__name__}\")\n",
    "\n",
    "            # Iterate over questions\n",
    "            output_valid = 0\n",
    "            tool_use = 0\n",
    "            error_messages = []\n",
    "            outputs = []\n",
    "\n",
    "            for kk in range(n_iter):\n",
    "                for ii in range(n_questions):\n",
    "                    parser = PydanticOutputParser(pydantic_object=pydantic_obj)\n",
    "                    prompt = prompt_format.partial(\n",
    "                        format_instructions=parser.get_format_instructions()\n",
    "                    )\n",
    "                    test_chain = prompt | llm_model | parser\n",
    "\n",
    "                    try:\n",
    "                        output = test_chain.invoke(dict(question=questions[ii]))\n",
    "                        assert isinstance(output, pydantic_obj)\n",
    "                        output_valid += 1\n",
    "                        outputs.append(output)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Invalid ouput ({type(e)})\")\n",
    "                        error_messages.append(f\"{type(e).__name__}, {e}\")\n",
    "\n",
    "            ss_results[model_name][pydantic_obj.__name__] = dict(\n",
    "                valid=output_valid / (n_iter * n_questions),\n",
    "                tool_use=tool_use / (n_iter * n_questions),\n",
    "                errors=error_messages,\n",
    "                outputs=outputs,\n",
    "            )\n",
    "    return ss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_support_by_model_op = run_experiment_with_op(\n",
    "    prompt_user_format, llm_models, n_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_support_by_model_op_jsonmode = run_experiment_with_op(\n",
    "    prompt_user_format, llm_models_jsonmode, n_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_support_by_model_op_system = run_experiment_with_op(\n",
    "    prompt_system_format, llm_models, n_iter\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
