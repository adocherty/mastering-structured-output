{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain to get structured outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    }
   ],
   "source": [
    " %xmode minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_api_key = \"<API KEY>\"\n",
    "claude_api_key = st.secrets[\"api_keys\"][\"ANTHROPIC_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a LLM model to run our structured output queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_model = ChatOllama(model=\"llama3.2\", temperature=0.5)\n",
    "llm_model = ChatAnthropic(model=\"claude-3-5-haiku-20241022\", api_key=claude_api_key)\n",
    "# llm_model = ChatOllama(model=\"nemotron-mini\", temperature=0.8)\n",
    "# llm_model = ChatOllama(model=\"gemma2\", temperature=0.8)\n",
    "# llm_model = ChatAnthropic(model=\"claude-3-5-haiku-20241022\", api_key=claude_api_key),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a zebra joke for you:\n",
      "\n",
      "Why do zebras always argue?\n",
      "Because they're always seeing things in black and white!\n"
     ]
    }
   ],
   "source": [
    "print(llm_model.invoke(\"Tell me a joke about zebras\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Structured output using the tool-calling API under the hood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Pydantic model and the output will be returned as a Pydantic object with validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: int = Field(description=\"How funny the joke is, from 1 to 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Function calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why do dogs make terrible dancers?' punchline='Because they have two left feet!' rating=7\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(Joke, method=\"function_calling\")\n",
    "\n",
    "try:\n",
    "    output = structured_llm.invoke(f\"Tell me a joke about dogs\")\n",
    "\n",
    "    if output is None:\n",
    "        print(\"Structured output call failed\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"  Parsing error \\n{type(e)}.__name__: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: JSON Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why are cats so good at keeping secrets?', punchline=\"Because they're purr-fect at not letting the cat out of the bag!\", rating=7)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(Joke, method=\"json_schema\")\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: JSON Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why are cats such terrible storytellers?' punchline=\"Because they only have one tale to tell - and it's always about themselves!\" rating=7\n"
     ]
    }
   ],
   "source": [
    "output_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "structured_llm = llm_model.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "try:\n",
    "    output = structured_llm.invoke(f\"Tell me a joke about cats\\n {format_instructions}\")\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"  Parsing error \\n{type(e)}.__name__: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the schema using a TypedDict parses the JSON output into a Python dict not a Pydantic object so there's no schema validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the monkey get kicked out of the library?',\n",
       " 'punchline': 'Because he was caught monkeying around!',\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class JokeTD(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], ..., \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm_model.with_structured_output(JokeTD, method=\"json_schema\")\n",
    "structured_llm.invoke(\"Tell me a joke about monkeys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can just extract the JSON Schema object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the reindeer go to the party?',\n",
       " 'punchline': \"Because he heard it was a 'hoof' event!\",\n",
       " 'rating': 4}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(\n",
    "    Joke.model_json_schema(), method=\"json_schema\"\n",
    ")\n",
    "structured_llm.invoke(\"Tell me a joke about raindeer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complicated structure with nested types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleResponse(title='The History of the State of Texas in America', context='Located in the south-central region of the United States, Texas has a rich and diverse history that spans thousands of years.', historical_timeline=['1519: Spanish explorer Francisco VÃ¡squez de Coronado arrives in present-day Texas', '1821: Mexico gains independence from Spain, and Texas becomes part of the new nation', '1835-1836: The Texas Revolution leads to the establishment of the Republic of Texas', '1845: Texas is annexed by the United States and becomes the 28th state'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ArticleResponse(BaseModel):\n",
    "    \"\"\"A clear and concise answer to the users question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    context: str = Field(\n",
    "        description=\"Provide a brief historical context to answer the question.\"\n",
    "    )\n",
    "    historical_timeline: list[str] = Field(\n",
    "        description=\"Provide a list of historical events relevant to the question\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = llm_model.with_structured_output(ArticleResponse)\n",
    "structured_llm.invoke(\"Tell me the history of the state of Texas in America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleResponse(title='History of Texas', context='Statehood and Beyond', historical_timeline=['Pre-Columbian Era (10,000 BCE - 1528 CE)', 'Spanish Colonial Period (1528-1821)', 'Mexican Independence and Early Republic (1821-1836)', 'Texas Revolution and Statehood (1836-1845)', 'Republic of Texas and Annexation by the US (1845-1861)', 'Civil War and Reconstruction (1861-1877)', \"Late 19th Century and World's Fair (1878-1900)\", '20th Century and Oil Boom (1901-1945)', 'Modern Era and Contemporary Issues (1946-Present)'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(ArticleResponse, method=\"json_schema\")\n",
    "structured_llm.invoke(\"Tell me the history of the state of Texas in America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Failed to parse ArticleResponse from completion {\"properties\": {\"title\": {\"description\": \"The title of the state and its history\", \"title\": \"Texas History\", \"type\": \"string\"}, \"context\": {\"description\": \"A brief historical context to answer the question.\", \"title\": \"Context\", \"type\": \"string\"}, \"historical_timeline\": {\"description\": \"Provide a list of historical events relevant to the question\", \"items\": {\"type\": \"string\"}, \"title\": \"Historical Timeline\", \"type\": \"array\"}}, \"required\": [\"title\", \"context\", \"historical_timeline\"]}. Got: 3 validation errors for ArticleResponse\ntitle\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ncontext\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nhistorical_timeline\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mOutputParserException\u001b[0m\u001b[0;31m:\u001b[0m Failed to parse ArticleResponse from completion {\"properties\": {\"title\": {\"description\": \"The title of the state and its history\", \"title\": \"Texas History\", \"type\": \"string\"}, \"context\": {\"description\": \"A brief historical context to answer the question.\", \"title\": \"Context\", \"type\": \"string\"}, \"historical_timeline\": {\"description\": \"Provide a list of historical events relevant to the question\", \"items\": {\"type\": \"string\"}, \"title\": \"Historical Timeline\", \"type\": \"array\"}}, \"required\": [\"title\", \"context\", \"historical_timeline\"]}. Got: 3 validation errors for ArticleResponse\ntitle\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ncontext\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nhistorical_timeline\n  Field required [type=missing, input_value={'properties': {'title': ... 'historical_timeline']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(ArticleResponse, method=\"json_mode\")\n",
    "output_parser = PydanticOutputParser(pydantic_object=ArticleResponse)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "structured_llm.invoke(\n",
    "    f\"Tell me the history of the state of Texas in America \\n {format_instructions}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of ways these different methods go wrong.\n",
    "\n",
    "To catch these different ways, I find it's useful to return the raw message so that the LLM response is available directly to see what happened. This can be done with `include_raw=True`.\n",
    "\n",
    "Then, we can have the following:\n",
    "\n",
    "- `output[\"parsing_error\"]` is not `None` if there was a parsing error, most likely the output did not conform to the schema\n",
    "\n",
    "- `output[\"parsed\"]` is `None` if there was an error returning any output (most common with Method 1, function calling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "title='The History of Texas in America' context='Located in the south-central region of the United States, Texas has a rich and diverse history dating back to its early days as a Spanish colony.' historical_timeline=['1821: Texas becomes part of Mexico after gaining independence from Spain', '1836: The Texas Revolution leads to the establishment of the Republic of Texas', '1845: Texas joins the United States as the 28th state', '1860s: Texas plays a significant role in the American Civil War', '1945: World War II ends with the defeat of Nazi Germany and Imperial Japan']\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(\n",
    "    ArticleResponse, method=\"function_calling\", include_raw=True\n",
    ")\n",
    "output = structured_llm.invoke(\"Tell me the history of the state of Texas in America\")\n",
    "\n",
    "if output[\"parsing_error\"] is not None:\n",
    "    print(\"Error: Parsing failed\")\n",
    "    print(output[\"parsing_error\"])\n",
    "    print(\"---\")\n",
    "    print(\"Raw output:\")\n",
    "    print(output[\"raw\"])\n",
    "elif output[\"parsed\"] is None:\n",
    "    print(\"Error: No output\")\n",
    "else:\n",
    "    print(\"Success!\")\n",
    "    print(output[\"parsed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly create the JSON schema object from the Pydantic object and we get the raw dict output without Pydantic validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Wombats are burrowing marsupials native to Australia.',\n",
       " 'historical_timeline': ['Prehistoric Era: Wombats evolve from ancient marsupial ancestors',\n",
       "  'Middle Ages: European settlers arrive in Australia and discover wombats',\n",
       "  '19th Century: Wombats become an important food source for Australian colonizers',\n",
       "  '20th Century: Conservation efforts begin to protect wombat habitats'],\n",
       " 'title': 'Wombats History'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm_js = llm_model.with_structured_output(\n",
    "    ArticleResponse.model_json_schema(), method=\"function_calling\"\n",
    ")\n",
    "structured_llm_js.invoke(\"Tell me the history of wombats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which models support what?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = {\n",
    "    \"Anthropic_Haiku\": ChatAnthropic(\n",
    "        model=\"claude-3-5-haiku-20241022\", api_key=claude_api_key\n",
    "    ),\n",
    "    \"Ollama_llama32\": ChatOllama(model=\"llama3.2\", temperature=1),\n",
    "    \"Ollama_nemotron\": ChatOllama(model=\"nemotron-mini\", temperature=1),\n",
    "    \"Ollama_gemma2\": ChatOllama(model=\"gemma2\", temperature=1),\n",
    "    \"Ollama_phi3\": ChatOllama(model=\"phi3\", temperature=1),\n",
    "    \"Ollama_phi4\": ChatOllama(model=\"phi4\", temperature=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ChatAnthropic(model='claude-3-5-haiku-20241022', anthropic_api_url='https://api.anthropic.com', anthropic_api_key=SecretStr('**********'), model_kwargs={})\n",
      "  Tool use support\n",
      "  JSON mode support\n",
      "  JSON schema support\n",
      "Model: ChatOllama(model='llama3.2', temperature=1.0)\n",
      "  Tool use support\n",
      "  JSON mode support\n",
      "  JSON schema support\n",
      "Model: ChatOllama(model='gemma2', temperature=1.0)\n",
      "  No tool use\n",
      "  JSON mode support\n",
      "  JSON schema support\n",
      "Model: ChatOllama(model='phi3', temperature=1.0)\n",
      "  No tool use\n",
      "  JSON mode support\n",
      "  JSON schema support\n",
      "Model: ChatOllama(model='phi3', temperature=1.0)\n",
      "  No tool use\n",
      "  JSON mode support\n",
      "  JSON schema support\n"
     ]
    }
   ],
   "source": [
    "structured_support = {}\n",
    "\n",
    "for llm_model in llm_models.values():\n",
    "    model_name = llm_model.__repr__()\n",
    "    print(f\"Model: {model_name}\")\n",
    "    ss_model = {}\n",
    "    try:\n",
    "        structured_llm = llm_model.with_structured_output(\n",
    "            ArticleResponse.model_json_schema(), method=\"function_calling\"\n",
    "        )\n",
    "        output = structured_llm.invoke(\"Tell the history of New Zealand\")\n",
    "        ss_model[\"function_calling\"] = True\n",
    "        print(\"  Tool use support\")\n",
    "    except Exception as e:\n",
    "        print(\"  No tool use\")\n",
    "\n",
    "    try:\n",
    "        structured_llm = llm_model.with_structured_output(\n",
    "            ArticleResponse.model_json_schema(), method=\"json_mode\"\n",
    "        )\n",
    "        output_parser = PydanticOutputParser(pydantic_object=ArticleResponse)\n",
    "        format_instructions = output_parser.get_format_instructions()\n",
    "        output = structured_llm.invoke(\n",
    "            f\"Tell the history of New Zealand \\n {format_instructions}\"\n",
    "        )\n",
    "        ss_model[\"json_mode\"] = True\n",
    "        print(\"  JSON mode support\")\n",
    "    except Exception as e:\n",
    "        print(\"  No JSON mode\")\n",
    "\n",
    "    try:\n",
    "        structured_llm = llm_model.with_structured_output(\n",
    "            ArticleResponse.model_json_schema(), method=\"json_schema\"\n",
    "        )\n",
    "        output = structured_llm.invoke(\"Tell the history of New Zealand\")\n",
    "        ss_model[\"json_schema\"] = True\n",
    "        print(\"  JSON schema support\")\n",
    "    except Exception as e:\n",
    "        print(\"  No JSON schema\")\n",
    "\n",
    "    structured_support[model_name] = ss_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under the hood: How Pydantic models are converted to JSONSchema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON schema representation is quite straightforward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Joke to tell user.',\n",
       " 'properties': {'setup': {'description': 'The setup of the joke',\n",
       "   'title': 'Setup',\n",
       "   'type': 'string'},\n",
       "  'punchline': {'description': 'The punchline to the joke',\n",
       "   'title': 'Punchline',\n",
       "   'type': 'string'},\n",
       "  'rating': {'description': 'How funny the joke is, from 1 to 10',\n",
       "   'title': 'Rating',\n",
       "   'type': 'integer'}},\n",
       " 'required': ['setup', 'punchline', 'rating'],\n",
       " 'title': 'Joke',\n",
       " 'type': 'object'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Joke.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the same schema is contained in the format instructions, expect for 'title' and 'type'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Joke to tell user.\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"description\": \"How funny the joke is, from 1 to 10\", \"title\": \"Rating\", \"type\": \"integer\"}}, \"required\": [\"setup\", \"punchline\", \"rating\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "print(output_parser.get_format_instructions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
