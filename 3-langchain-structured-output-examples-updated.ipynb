{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain to get structured outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTHROPIC_API_KEY = \"<API KEY>\"\n",
    "FIREWORKS_API_KEY = \"<API KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a LLM model to run our structured output queries. Use a temperature of 0 to improve structured output generation (but at the cost of \"creativity\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a LLM model below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    temperature=temperature,\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    ")\n",
    "# llm_model = ChatOllama(model=\"llama3.2\", temperature=temperature)\n",
    "# llm_model = ChatFireworks(\n",
    "#     model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "#     temperature=temperature,\n",
    "#     api_key=FIREWORKS_API_KEY,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the zebra refuse to play poker?\n",
      "\n",
      "Because he always got striped of his money! (get it?)\n"
     ]
    }
   ],
   "source": [
    "print(llm_model.invoke(\"Tell me a joke about zebras\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured output methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Pydantic model and the output will be returned as a Pydantic object with validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: int = Field(description=\"How funny the joke is, from 1 to 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 1: Function calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Because he was feeling ruff!' punchline='Why did the dog go to the vet?' rating=8\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(Joke, method=\"function_calling\")\n",
    "\n",
    "try:\n",
    "    output = structured_llm.invoke(f\"Tell me a joke about dogs\")\n",
    "\n",
    "    if output is None:\n",
    "        print(\"Structured output call failed\")\n",
    "    else:\n",
    "        print(output)\n",
    "except Exception as e:\n",
    "    print(f\"  Parsing error \\n{type(e)}.__name__: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 2: JSON Mode\n",
    "\n",
    "Note for JSON Mode we need to include the structure in the prompt as well as providing it to the `.with_structured_output` method. Here I don't provide the Pydantic model purely as this method often fails schema validation, and it's instructive to see the raw JSON output from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'setup': {'title': 'Why did the rabbit go to the doctor?', 'description': 'Because it had hare loss!', 'type': 'string'}, 'punchline': {'title': 'Punchline', 'description': 'The punchline to the joke', 'type': 'string'}, 'rating': {'title': 'Rating', 'description': 'How funny the joke is, from 1 to 10', 'type': 'integer'}}, 'required': ['setup', 'punchline', 'rating']}\n"
     ]
    }
   ],
   "source": [
    "output_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "structured_llm = llm_model.with_structured_output(\n",
    "    Joke.model_json_schema(), method=\"json_mode\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    output = structured_llm.invoke(\n",
    "        f\"Tell me a joke about rabbits\\n {format_instructions}\"\n",
    "    )\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(f\"  Parsing error \\n{type(e)}.__name__: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 3: JSON Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the frog go to the doctor?' punchline='Because it had a ribbiting cough!' rating=8\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(Joke, method=\"json_schema\")\n",
    "output = structured_llm.invoke(\"Tell me a joke about frogs\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vallidation of the returned JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Pydantic object directly will return a chain that includes a `PydanticOutputParser` that uses Pydantic to validate the schema of the data.\n",
    "\n",
    "If this is not desired behaviour then defining the schema using a TypedDict parses the JSON output into a Python dict not a Pydantic object so there's no schema validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the monkey get kicked out of the library?',\n",
       " 'punchline': 'Because he was caught monkeying around!',\n",
       " 'rating': 8}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class JokeTD(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], ..., \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm_model.with_structured_output(JokeTD, method=\"json_schema\")\n",
    "structured_llm.invoke(\"Tell me a joke about monkeys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have a Pydantic object specifying the schema but want to validate or fix the data yourself, you can extract the JSON Schema object from the Pydantic model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': 'Why did the reindeer go to the party?',\n",
       " 'punchline': \"Because he heard it was going to be a 'hoof' event!\",\n",
       " 'rating': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(\n",
    "    Joke.model_json_schema(), method=\"json_schema\"\n",
    ")\n",
    "structured_llm.invoke(\"Tell me a joke about raindeer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of ways these different methods go wrong.\n",
    "\n",
    "To catch these different ways, I find it's useful to return the raw message so that the LLM response is available directly to see what happened. This can be done with `include_raw=True`.\n",
    "\n",
    "Then, we can have the following:\n",
    "\n",
    "- `output[\"parsing_error\"]` is not `None` if there was a parsing error, most likely the output did not conform to the schema\n",
    "\n",
    "- `output[\"parsed\"]` is `None` if there was an error returning any output (most common with Method 1, function calling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleResponse(BaseModel):\n",
    "    \"\"\"A clear and concise answer to the users question.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"Title of the article\")\n",
    "    context: str = Field(\n",
    "        description=\"Provide a brief historical context to answer the question.\"\n",
    "    )\n",
    "    historical_timeline: list[str] = Field(\n",
    "        description=\"Provide a list of historical events relevant to the question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Parsing failed\n",
      "Failed to parse ArticleResponse from completion {}. Got: 3 validation errors for ArticleResponse\n",
      "title\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "context\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "historical_timeline\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "---\n",
      "Raw output:\n",
      "content='{} \\n\\n   \\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n' additional_kwargs={} response_metadata={} id='run-7fc5fddc-5576-4a3a-9149-4ec21e72c5bf-0'\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm_model.with_structured_output(\n",
    "    ArticleResponse, method=\"json_mode\", include_raw=True\n",
    ")\n",
    "output = structured_llm.invoke(\"Tell me the history of the state of Texas in America\")\n",
    "\n",
    "if output[\"parsing_error\"] is not None:\n",
    "    print(\"Error: Parsing failed\")\n",
    "    print(output[\"parsing_error\"])\n",
    "    print(\"---\")\n",
    "    print(\"Raw output:\")\n",
    "    print(output[\"raw\"])\n",
    "elif output[\"parsed\"] is None:\n",
    "    print(\"Error: No output\")\n",
    "else:\n",
    "    print(\"Success!\")\n",
    "    print(output[\"parsed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly create the JSON schema object from the Pydantic object and we get the raw dict output without Pydantic validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm_js = llm_model.with_structured_output(\n",
    "    ArticleResponse.model_json_schema(), method=\"function_calling\"\n",
    ")\n",
    "structured_llm_js.invoke(\"Tell me the history of wombats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood: How Pydantic models are converted to JSONSchema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON schema representation is quite straightforward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Joke to tell user.',\n",
       " 'properties': {'setup': {'description': 'The setup of the joke',\n",
       "   'title': 'Setup',\n",
       "   'type': 'string'},\n",
       "  'punchline': {'description': 'The punchline to the joke',\n",
       "   'title': 'Punchline',\n",
       "   'type': 'string'},\n",
       "  'rating': {'description': 'How funny the joke is, from 1 to 10',\n",
       "   'title': 'Rating',\n",
       "   'type': 'integer'}},\n",
       " 'required': ['setup', 'punchline', 'rating'],\n",
       " 'title': 'Joke',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Joke.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the same schema is contained in the format instructions, expect for 'title' and 'type'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Joke to tell user.\", \"properties\": {\"setup\": {\"description\": \"The setup of the joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"The punchline to the joke\", \"title\": \"Punchline\", \"type\": \"string\"}, \"rating\": {\"description\": \"How funny the joke is, from 1 to 10\", \"title\": \"Rating\", \"type\": \"integer\"}}, \"required\": [\"setup\", \"punchline\", \"rating\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "print(output_parser.get_format_instructions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
